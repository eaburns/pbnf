\documentclass{article}
\usepackage{aaai}
\usepackage{graphicx}

% Big margins for now so people can take notes/scribbles.
%\usepackage{fullpage}

\title{A Survey of Parallel Search Algorithms}
\author{Seth Lemons \\
Department of Computer Science \\
University of New Hampshire \\
Durham, NH 03824 USA \\
seth.lemons@unh.edu}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
State space search is an intensive process requiring the examination of many different paths before a solution is found. Any success in doing different parts of a search at the same time can be of great benefit in our increasingly multi-core world. However, states are often interrelated in ways that make parallelization very difficult, causing many proposed algorithms to suffer from synchronization issues or the inability to properly portion out the work. This work attempts to examine the strengths and weaknesses of the most promising parallel search algorithms. We will attempt to determine how well they perform compared to serial algorithms, how scalable they are, and whether they generalize to more than a limited number of domains.
\end{abstract}

\section{Introduction}
Given the large amounts of work that have to be done in state space search, the ability to do more of it at the same time is a tool that could greatly improve the ability to find fast solutions. With multi-core processors becoming more common and the numbers of cores increasing, doing work in parallel has become much more of a practical consideration. However, designing parallel algorithms is not a trivial activity. Many attempts have been made, and they vary in applicability to given domains, speed in comparison to serial algorithms, optimality of solutions found, and scalability to larger numbers of processor cores. In this study, we attempt to evaluate many parallel algorithms on search problems which can be solved using only system memory. We examine their performance in a threaded setting where all threads share access to main memory.

The algorithms examined are a naive parallel implementation of A*, Parallel Retracting A* \cite{evett:pra}, a parallel version of K Best First Search \cite{felner:kbf}, two versions of Parallel Structured Duplicate Detection \cite{zhou:psd}, and Parallel Best NBlock First [CITE HERE]. While the details vary, most of the above are best first and all are capable of handling problems which can be fit in memory. Only some can handle larger problems which require external memory or limits on total memory use.
\section{Algorithms Evaluated}
\subsection{Parallel A*}
The A* algorithm \cite{hart:fbh} is guaranteed to find optimal solutions while searching the least amount of the state space possible with the given heuristic by evaluating the single best node at every step. Because the algorithm requires that the single best node be expanded at each step, it is not possible to directly parallelize the algorithm as it stands. However, our Parallel A* (PA*) algorithm allows threads to select the current best node, ignoring nodes currently being expanded or those created but not yet added to the open list. This means that the first solution returned will not necessarily be optimal and the number of nodes expanded may be greater than serial A*. In addition, threads will have to wait on each other at many points to get nodes off of the open list and insert into the closed list. It is possible that this algorithm, by virtue of doing more than one expansion at a time, could be faster than serial A*. What we will see in our results is that the extra work done and the contention on the open and closed lists exceeds the benefits of parallel execution and makes the algorithm not only slower than A*, but entirely unscalable when more threads are used.
\subsection{Parallel Retracting A*}

%ignore retracting
%contention sucks here, too
\subsection{Parallel KBFS}

%waiting on last processor to finish takes time
\subsection{Parallel Structured Duplicate Detection (PSDD)}

%layers cause issues for non-unit
\subsection{Best First Parallel Structured Duplicate Detection (BFPSDD)}
\cite{zhou:sdd}
%only benefit from sorting in last layer
\subsection{Parallel Best NBlock First (PBNF)}
%livelock issue
%size of abstraction can cause issues
%\subsection{Safe Parallel Best NBlock First (Safe PBNF)}
\section{Experimental Results}
%\section{Related Work}
%maybe mention PWS and DTS
\section{Conclusion}

\bibliography{master}
\bibliographystyle{aaai}

\end{document}
