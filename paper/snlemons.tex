\documentclass{article}
\usepackage{aaai}
\usepackage{graphicx}

% Big margins for now so people can take notes/scribbles.
%\usepackage{fullpage}

\title{A Survey of Parallel Search Algorithms}
\author{Seth Lemons \\
Department of Computer Science \\
University of New Hampshire \\
Durham, NH 03824 USA \\
seth.lemons@unh.edu}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
State space search is an intensive process requiring the examination of many different paths before a solution is found. Any success in doing different parts of a search at the same time can be of great benefit in our increasingly multi-core world. However, states are often interrelated in ways that make parallelization very difficult, causing many proposed algorithms to suffer from synchronization issues or the inability to properly portion out the work. This work attempts to examine the strengths and weaknesses of the most promising parallel search algorithms. We will attempt to determine how well they perform compared to serial algorithms, how scalable they are, and whether they generalize to more than a limited number of domains.
\end{abstract}

\section{Introduction}
Given the large amounts of work that have to be done in state space search, the ability to do more of it at the same time is a tool that could greatly improve the ability to find fast solutions. With multi-core processors becoming more common and the numbers of cores increasing, doing work in parallel has become much more of a practical consideration. However, designing parallel algorithms is not a trivial activity. Many attempts have been made, and they vary in applicability to given domains, speed in comparison to serial algorithms, optimality of solutions found, and scalability to larger numbers of processor cores. In this study, we attempt to evaluate many parallel algorithms on search problems which can be solved using only system memory. We examine their performance in a threaded setting where all threads share access to main memory.

The algorithms examined are a naive parallel implementation of A*, Parallel Retracting A* \cite{evett:pra}, a parallel version of K Best First Search \cite{felner:kbf}, two versions of Parallel Structured Duplicate Detection \cite{zhou:psd}, and Parallel Best NBlock First [CITE HERE]. While the details vary, most of the above are best first and all are capable of handling problems which can be fit in memory. Only some can handle larger problems which require external memory or limits on total memory use.
\section{Algorithms Evaluated}
\subsection{Parallel A*}
The A* algorithm \cite{hart:fbh} is guaranteed to find optimal solutions while searching the least amount of the state space possible with the given heuristic by evaluating the single best node at every step. Because the algorithm requires that the single best node be expanded at each step, it is not possible to directly parallelize the algorithm as it stands. However, our Parallel A* (PA*) algorithm allows threads to select the current best node, ignoring nodes currently being expanded or those created but not yet added to the open list. This means that the first solution returned will not necessarily be optimal and the number of nodes expanded may be greater than serial A*. In addition, threads will have to wait on each other at many points to get nodes off of the open list and insert into the closed list. It is possible that this algorithm, by virtue of doing more than one expansion at a time, could be faster than serial A*. What we will see in our results is that the extra work done and the contention on the open and closed lists exceeds the benefits of parallel execution and makes the algorithm slower than A* for the numbers of threads used.
\subsection{Parallel Retracting A*}
To avoid constant contention on a single open and closed list, Parallel Retracting A* (PRA*) was designed with each thread being given its own lists to search. The determination of which nodes belong to which threads can be done by any hash, although ideally nodes will be hashed in such a way as to split them evenly among the threads. The original design of PRA* was focused on message passing schemes for systems without shared memory, but our implementation uses shared memory to avoid the overhead of messages. In additon, the retracting property of PRA* is a method by which nodes can be condensed back into their parents to allow the algorithm to operate within a memory bound. Because we examined only problems which were known to fit into memory (and because retracting introduces a known deadlock condition), retraction was not built into our implementation. PRA* still finds a suboptimal first solution and may also expand more nodes than A*, but memory contention may be reduced because threads should not always be putting nodes into the same lists at the same time.
\subsection{Parallel KBFS}
One method of reducing contention is to globally synchronize where possible. The K Best First Search algorithm (KBFS) takes the K best nodes from the open list and evaluates them before looking at the open list again. This work can be trivially parallelized by allowing a main thread to pull nodes off the open list and give them to other threads to be expanded. Some contention still occurs when inserting nodes into the open and closed lists, and some threads may spend time waiting on others to finish before the main thread can distribute more work. Like the other algorithms, KBFS can do more work than A* and it still runs the risk of contention in some places. Furthermore, the coordination between the threads can cause inefficiency in various ways. In many cases, the overhead of putting threads to sleep and waking them can add a substantial amount of overhead compared to algorithms in which all threads are constantly attemptimg to do work.
\subsection{Parallel Structured Duplicate Detection (PSDD)}
While PRA* partitions the state space to avoid some contention, the threads must still access each other's data to insert nodes and check for duplicates. The Parallel Structured Duplicate Detection algorithm (PSDD) uses a formal abstraction of the state space to separate nodes into categories and further avoid contention. The abstraction looks much like the original state space but smaller, with all original states mapping to only one abstract state (or nblock) and each nblock having a known set of successors. This successor set is called the duplicate detection scope because exclusive access to this set allows a thread to expand nodes from the original nblock's open list, check the closed list of the successor nblock to which a child node belongs for duplicates, and add the children to the appropriate open lists without obtaining a lock.

This algorithm requires a more in-depth analysis of the state space than a has function, but provides more discreet groupings to which threads can be given exclusive access. The abstraction is maintained as a hash table mapping nblocks to $\sigma$ values representing the number of successors of a given state which are currently in use. When $\sigma$ reaches zero, the nblock and its entire duplicate detection scope are free. A thread which has no nblock to search from takes the lock on the hash table, finds an nblock $b$ with $\sigma$ of zero, and increments the $\sigma$ values of the predeccessors of all nblocks in the duplicate detection scope of $b$. When the thread is done with $b$, the lock is acquired and the $\sigma$ values of the predeccessors of all nblocks in $b$'s duplicate detection scope are decremented. Exclusive access to $b$ and its duplicate detection scope can be guaranteed because for another thread to be able to access $b$ or any nblock in its duplicate detection scope, their sigma values would have to be decremented by the thread currently using them. %check that this description makes sense

The underlying search algorithm originally described proceeds in a breadth first fashion, expanding all nodes from an nblock that belong to the current layer and proceeding to the next layer only after all nodes in the previous layer have been expanded. Our implementation utilizes breadth first heuristic search \cite{zhou:bhs}, which requires an upper bound on solution cost. We acquire this bound by first performing a weighted A* and storing the cost of the solution returned. The time spent on the weighted A* search is factored into all results. While the breadth first search method guarantees optimal solutions in unit cost worlds, this not remain true in non-unit cost worlds. In addition, outside of a unit cost setting, the number of nodes assigned to a given layer can be very small, causing the time spent switching between nblocks and layers to be much greater.
\subsection{Best First Parallel Structured Duplicate Detection (BFPSDD)}
One adaptation to PSDD suggested by Zhou and Hansen was to search layers in a best first order \cite{zhou:sdd} by expanding from the open list of each nblock based on f. The search still expands all nodes in a layer before moving to the next, but the ordering within that layer is different. Because the solution should be one of the lowest cost nodes on its layer, the search can be stopped before fully exploring that layer. Essentially, less work may be performed at the last (and largest) layer if the ordering is good, but nothing is gained at layers without a solution. This modification reduces some of the search cost, but does not resolve the issues with layers in a non-unit cost setting. Another modification was proposed to allow for grouping ranges of f values into a single layer, but that was not included in our implementation.
\subsection{Parallel Best NBlock First (PBNF)}

%livelock issue
%size of abstraction can cause issues
%\subsection{Safe Parallel Best NBlock First (Safe PBNF)}
\section{Experimental Results}
%\section{Related Work}
%maybe mention PWS and DTS
\section{Future Work}
\section{Conclusion}

\bibliography{master}
\bibliographystyle{aaai}

\end{document}
